# -*- coding: utf-8 -*-
"""
Created on Thu Nov 21 19:58:23 2019

The function of this program is to download reports(PDF) from the Juchao(巨潮) news center

@author: Gareth
@e-mail:gareth@stu.xmu.edu.cn
"""

import requests
import time
import pandas as pd
import random
import os

def get_pdf_address(pageNum):
    url = 'http://www.cninfo.com.cn/new/hisAnnouncement/query'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3741.400 QQBrowser/10.5.3863.400'}
    pageNum = int(pageNum)
    payload = {'pageNum': pageNum,
               'pageSize': '30',
               'tabName': 'fulltext',        ##爬取PDF全文
               'column': 'szse',               ##沪市此处参数为sse 深市szse
               'stock': '',
               'searchkey': '',                ##支持任意关键词搜索，如想要爬取社会责任报告，则搜索“社会责任”，项目自动爬取标题带社会责任的全部报告
               'secid': '',
               'plate': 'szmb',                   ##想要爬取沪市上市公司报告就填sh 深市填sz
               'category': '',  ##此处默认爬取年报 category_ndbg_szsh 如使用关键词搜索功能，该项为空
               'trade': '',
               'seDate': '2019-12-18~2019-12-18',            #支持设定爬取时间范围，直接在此处修改日期即可 
               'sortName':'', 
               'sortType': '',
               'isHLtitle': 'true'}         
    response = requests.post(url,headers=headers,data=payload)
    print()
    result = response.json()
    #print(result)
    return result


pdf_infor= pd.DataFrame(columns =['secCode','secName','url','title'])  #创建一个DataFrame储存爬取信息
count = 0
url_head ='http://static.cninfo.com.cn/'
for i in range(2,3):                                ##此处设置爬取页数
    print("爬取报告下载地址第{}页".format(i))
    result = get_pdf_address(i)
    num = len(result['announcements'])
    for each in range(num):
        # each = 1
        pdf_infor.at[count,'secCode'] = result['announcements'][each]['secCode']
        pdf_infor.at[count,'secName'] = result['announcements'][each]['secName']
        pdf_infor.at[count,'url'] = url_head + result['announcements'][each]['adjunctUrl']
        pdf_infor.at[count,'title'] = result['announcements'][each]['announcementTitle']
        #pdf_infor.at[count,'publishTime'] = result['data'][each]['publishTime']
        count += 1
    print('获取完成')
    time.sleep(random.uniform(1,2)) # 控制访问速度   


pdf_infor['Year'] = pdf_infor['title'].str.extract('([0-9]{4})')  # 提取title中字符串获取年份
file_path= 'H:\\spider\\juchao\\report\\'  # 这里设置存放路径
headers ={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3732.400 QQBrowser/10.5.3819.400'}
    
for each in range(pdf_infor.shape[0]):
    Stkcd = pdf_infor.at[each,'secCode']
    firm_name = pdf_infor.at[each,'title'].replace("*","")
    Year = pdf_infor.at[each,'Year']
    print("开始下载{}，股票代码{}".format(firm_name,Stkcd))
    file_name = "{}{}{}.pdf".format(Stkcd,firm_name,Year)
    file_full_name = os.path.join(file_path,file_name)
    # print(file_full_name)
    pdf_url = pdf_infor.at[each,'url']
    rs = requests.get(pdf_url,headers= headers,stream=True)
    with open(file_full_name, "wb") as fp:
        for chunk in rs.iter_content(chunk_size=10240):
            if chunk:
                fp.write(chunk)
    time.sleep(random.uniform(1,2)) # 控制访问速度
    print("===================下载完成=========================")
